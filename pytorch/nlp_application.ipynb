{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Natural Language Processing\n",
    "Welcome to NLP. NLP aims to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
    "Applications of NLP range from sentiment analysis, machine translation, chatbots, speech recognition, text summarization, and information retrieval.\n",
    "In this notebook, we'll dive into the world of text analysis.\n",
    "We will explore ways to extract meaning from text, and build a model that can differentiate between positive and negative sentiment in movie reviews.\n",
    "We'll be using a simplistic technique called Bag of Words,\n",
    " which involves representing text as numerical vectors of words represented their frequency and index.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "To start off let's get some data. The data we are going to use is from the IMDb Dataset.\n",
    "The IMDB dataset is a large dataset of movie reviews from the website IMDb.\n",
    "It contains 50,000 movie reviews, half of which are labeled as positive and half as negative,\n",
    "and is often used as a benchmark dataset for natural language processing tasks.\n",
    "Grab the dataset from [kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset into a pandas dataframe\n",
    "# try using 20 newsgroups instead: https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "# switch to pytorch lightning: https://lightning.ai/docs/pytorch/stable/\n",
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Let's see what's in it!\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "In order to get usable data, we must transform the data to be suitable for analysis.\n",
    "We'll be using some regular expression to clean out unwanted strings and\n",
    "the `CountVectorizer` from scikit-learn to transform the collection of text\n",
    " into a matrix of token counts where each row represents a document and\n",
    "  each column represents a unique word in the document collection.\n",
    "  Let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('One of the other reviewers has mentioned that after watching just 1 Oz '\n",
      " \"episode you'll be hooked. They are right, as this is exactly what happened \"\n",
      " 'with me.<br /><br />The first thing that struck me about Oz was its '\n",
      " 'brutality and unflinching scenes of violence, which set in right from the '\n",
      " 'word GO. Trust me, this is not a show for the faint hearted or timid. This '\n",
      " 'show pulls no punches with regards to drugs, sex or violence. Its is '\n",
      " 'hardcore, in the classic use of the word.<br /><br />It is called OZ as that '\n",
      " 'is the nickname given to the Oswald Maximum Security State Penitentary. It '\n",
      " 'focuses mainly on Emerald City, an experimental section of the prison where '\n",
      " 'all the cells have glass fronts and face inwards, so privacy is not high on '\n",
      " 'the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, '\n",
      " 'Christians, Italians, Irish and more....so scuffles, death stares, dodgy '\n",
      " 'dealings and shady agreements are never far away.<br /><br />I would say the '\n",
      " 'main appeal of the show is due to the fact that it goes where other shows '\n",
      " \"wouldn't dare. Forget pretty pictures painted for mainstream audiences, \"\n",
      " \"forget charm, forget romance...OZ doesn't mess around. The first episode I \"\n",
      " \"ever saw struck me as so nasty it was surreal, I couldn't say I was ready \"\n",
      " 'for it, but as I watched more, I developed a taste for Oz, and got '\n",
      " 'accustomed to the high levels of graphic violence. Not just violence, but '\n",
      " \"injustice (crooked guards who'll be sold out for a nickel, inmates who'll \"\n",
      " 'kill on order and get away with it, well mannered, middle class inmates '\n",
      " 'being turned into prison bitches due to their lack of street skills or '\n",
      " 'prison experience) Watching Oz, you may become comfortable with what is '\n",
      " 'uncomfortable viewing....thats if you can get in touch with your darker '\n",
      " 'side.')\n",
      "('One of the other reviewers has mentioned that after watching just   Oz '\n",
      " \"episode you'll be hooked  They are right  as this is exactly what happened \"\n",
      " 'with me   The first thing that struck me about Oz was its brutality and '\n",
      " 'unflinching scenes of violence  which set in right from the word GO  Trust '\n",
      " 'me  this is not a show for the faint hearted or timid  This show pulls no '\n",
      " 'punches with regards to drugs  sex or violence  Its is hardcore  in the '\n",
      " 'classic use of the word   It is called OZ as that is the nickname given to '\n",
      " 'the Oswald Maximum Security State Penitentary  It focuses mainly on Emerald '\n",
      " 'City  an experimental section of the prison where all the cells have glass '\n",
      " 'fronts and face inwards  so privacy is not high on the agenda  Em City is '\n",
      " 'home to many  Aryans  Muslims  gangstas  Latinos  Christians  Italians  '\n",
      " 'Irish and more    so scuffles  death stares  dodgy dealings and shady '\n",
      " 'agreements are never far away   I would say the main appeal of the show is '\n",
      " \"due to the fact that it goes where other shows wouldn't dare  Forget pretty \"\n",
      " 'pictures painted for mainstream audiences  forget charm  forget romance   OZ '\n",
      " \"doesn't mess around  The first episode I ever saw struck me as so nasty it \"\n",
      " \"was surreal  I couldn't say I was ready for it  but as I watched more  I \"\n",
      " 'developed a taste for Oz  and got accustomed to the high levels of graphic '\n",
      " \"violence  Not just violence  but injustice  crooked guards who'll be sold \"\n",
      " \"out for a nickel  inmates who'll kill on order and get away with it  well \"\n",
      " 'mannered  middle class inmates being turned into prison bitches due to their '\n",
      " 'lack of street skills or prison experience  Watching Oz  you may become '\n",
      " 'comfortable with what is uncomfortable viewing    thats if you can get in '\n",
      " 'touch with your darker side ')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# before\n",
    "pprint(df.iloc[0]['review'])\n",
    "\n",
    "# remove non characters from review\n",
    "regex = re.compile('<\\\\w+ /?>|[^\\\\w \\']|\\\\d|_')\n",
    "df['review'].replace(regex, ' ', regex=True, inplace=True)\n",
    "\n",
    "# after\n",
    "pprint(df.iloc[0]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# stop_words='english' removes common English words like \"a\" or \"the' from the text\n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True, max_df=.5, min_df=10)\n",
    "review_bow = vectorizer.fit_transform(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x25453 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4131900 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aaa', 'aag', ..., 'zulu', 'zuniga', 'über'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see some of the tokens it collected\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_bow[23001].toarray().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Now let's make it into a pytorch `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pandas.core.series import Series\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    regex = re.compile('<\\\\w+ /?>|[^\\\\w \\']|\\\\d|_')\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "        self.vectorizer = CountVectorizer(stop_words='english', lowercase=True, max_df=.5, min_df=10)\n",
    "\n",
    "        # clean data\n",
    "        self.df['review'].replace(IMDBDataset.regex, ' ', regex=True, inplace=True)\n",
    "\n",
    "        # fit vectorizer\n",
    "        self.bows = self.vectorizer.fit_transform(self.df['review'])\n",
    "        \n",
    "        # map targets\n",
    "        self.sentiments = self.df.sentiment.map({\n",
    "            'negative': 0,\n",
    "            'positive': 1\n",
    "        }).values\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        X = self.bows[index].toarray().squeeze().astype(np.float32)\n",
    "        Y = self.sentiments[index].astype(np.float32)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    @property\n",
    "    def classes(self):\n",
    "        return 'negative', 'positive'\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = IMDBDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 1.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Next, let's make a super simple logistic regression model. The model should receive a tensor of term frequencies and output a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        return torch.sigmoid(self.linear(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(dataset.vocab_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters\n",
    "Set some hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 16\n",
    "lr = 3e-2\n",
    "num_folds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "Before we get to the training loop, let's understand cross-validation.\n",
    "Cross-validation is a technique used to evaluate a machine learning\n",
    "model's performance by splitting the dataset into multiple subsets or folds.\n",
    "The model is trained on a portion of the data and tested on the remaining fold, which is repeated for each fold.\n",
    "The overall performance is then calculated by averaging the performance of each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# k-fold cross validation\n",
    "kf = KFold(n_splits=num_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "We'll need a way to score the performance of our model.\n",
    "We'll benchmark our model with `AUROC` from torchmetrics.\n",
    "The `AUROC` score summarizes the Receiver Operating Characteristic Curve\n",
    " into a single number that describes the performance of a model for multiple thresholds at the same time.\n",
    " Notably, an `AUROC` score of 1 is a perfect score and an `AUROC` score of 0.5 corresponds to random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchmetrics import AUROC\n",
    "\n",
    "metric = AUROC(task='binary').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Our training loop performs k-fold cross validation, with each fold iterating through training and evaluating a logistic regression model on a binary classification task, with the `Adam` optimizer and `BCELoss` function, and computing the average train and test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch.optim import Adam, SGD\n",
    "from copy import deepcopy\n",
    "\n",
    "def cross_validate(model: nn.Module, dataset: Dataset, *, num_folds=3, epochs=3, lr=1e-3, batch_size=16):\n",
    "\n",
    "    # keep track of model scores\n",
    "    scores: List[float] = []\n",
    "\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "    metric = AUROC(task='binary').to(device)\n",
    "\n",
    "    for fold, (train_indices, test_indices) in enumerate(kf.split(dataset)):\n",
    "\n",
    "        # make samplers that samples elements randomly from a given list of indices without replacement.\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "        # data loaders\n",
    "        train_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=train_sampler,\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=test_sampler,\n",
    "        )\n",
    "\n",
    "        # re-initialize model\n",
    "        # need to do this, otherwise model will pick up\n",
    "        # where it left off on last cross-validation fold\n",
    "        for layer in model.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "\n",
    "        # loss function\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = SGD(model.parameters(), lr=lr)\n",
    "\n",
    "        # training loop\n",
    "        for epoch in range(epochs):\n",
    "            total_train_loss = 0\n",
    "            total_test_loss = 0\n",
    "\n",
    "            # training\n",
    "            print('training ', end='')\n",
    "            model.train()\n",
    "            for i, (X, Y) in enumerate(train_loader):\n",
    "                # print progress\n",
    "                if i % (len(train_loader) // 10) == 0:\n",
    "                    print('.', end='')\n",
    "\n",
    "                # forward pass\n",
    "                outputs: torch.Tensor = model(X.to(device)).squeeze()\n",
    "\n",
    "                # calculate loss\n",
    "                loss: torch.Tensor = criterion(outputs, Y.to(device))\n",
    "\n",
    "                # zero out accumulated gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # backpropagation\n",
    "                loss.backward()\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "                # update weights and biases\n",
    "                optimizer.step()\n",
    "\n",
    "            # testing\n",
    "            print('\\ntesting  ', end='')\n",
    "            model.eval()\n",
    "            with torch.inference_mode():\n",
    "                total_score = 0\n",
    "                for i, (X, Y) in enumerate(test_loader):\n",
    "                    # print progress\n",
    "                    if i % (len(test_loader) // 10) == 0:\n",
    "                        print('.', end='')\n",
    "\n",
    "                    outputs: torch.Tensor = model(X.to(device)).squeeze()\n",
    "                    loss: torch.Tensor = criterion(outputs, Y.to(device))\n",
    "\n",
    "                    # update loss and scores\n",
    "                    total_test_loss += loss.item()\n",
    "                    total_score += metric(outputs, Y.to(device)).item()\n",
    "            scores.append(total_score / len(test_loader))\n",
    "            print(f'\\nFold {fold} | Epoch {epoch} | train loss {total_train_loss / len(train_loader):.2f} | '\n",
    "                  f'test loss {total_test_loss / len(test_loader):.2f} | '\n",
    "                  f'test auroc {total_score / len(test_loader):.2f}')\n",
    "        print(f'\\n{\"-\" * 90}\\n')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ...........\n",
      "testing  ...........\n",
      "Fold 0 | Epoch 0 | train loss 0.42 | test loss 0.35 | test auroc 0.93\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 0 | Epoch 1 | train loss 0.32 | test loss 0.32 | test auroc 0.94\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 0 | Epoch 2 | train loss 0.29 | test loss 0.31 | test auroc 0.95\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 1 | Epoch 0 | train loss 0.42 | test loss 0.36 | test auroc 0.93\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 1 | Epoch 1 | train loss 0.32 | test loss 0.33 | test auroc 0.94\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 1 | Epoch 2 | train loss 0.29 | test loss 0.32 | test auroc 0.94\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 2 | Epoch 0 | train loss 0.42 | test loss 0.35 | test auroc 0.93\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 2 | Epoch 1 | train loss 0.32 | test loss 0.32 | test auroc 0.94\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 2 | Epoch 2 | train loss 0.29 | test loss 0.31 | test auroc 0.95\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "scores = cross_validate(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    num_folds=num_folds,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Interval\n",
    "Calculate range of values that is likely to contain\n",
    "the true population parameter with a certain level of\n",
    "confidence based on a sample of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# confidence interval function\n",
    "def confidence_interval(data: List[float]):\n",
    "    sem = stats.sem(data)\n",
    "    if sem == 0:\n",
    "        return data[0], data[0]\n",
    "    return stats.t.interval(confidence=.95, df=len(data)-1, loc=np.mean(data), scale=sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9369050515870934, 0.9453037602403791)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_interval(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement\n",
    "Now let's try to improve our model. Let's add more non-linearity with ReLU activation function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        return torch.sigmoid(self.relu(self.linear(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ...........\n",
      "testing  ...........\n",
      "Fold 0 | Epoch 0 | train loss 0.56 | test loss 0.53 | test auroc 0.93\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 0 | Epoch 1 | train loss 0.51 | test loss 0.52 | test auroc 0.93\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 0 | Epoch 2 | train loss 0.49 | test loss 0.51 | test auroc 0.94\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 1 | Epoch 0 | train loss 0.56 | test loss 0.53 | test auroc 0.93\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 1 | Epoch 1 | train loss 0.51 | test loss 0.51 | test auroc 0.93\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 1 | Epoch 2 | train loss 0.49 | test loss 0.51 | test auroc 0.93\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 2 | Epoch 0 | train loss 0.56 | test loss 0.53 | test auroc 0.93\n",
      "training ...........\n",
      "testing  ......."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/envs/pytorch_workshop/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\n",
      "Fold 2 | Epoch 1 | train loss 0.51 | test loss 0.51 | test auroc 0.93\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 2 | Epoch 2 | train loss 0.49 | test loss 0.51 | test auroc 0.93\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9287067517338505, 0.9333876505285355)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(len(dataset.vectorizer.get_feature_names_out()), 1).to(device)\n",
    "scores = cross_validate(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    num_folds=num_folds,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr\n",
    ")\n",
    "confidence_interval(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try increasing the model complexity with more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, hidden_units: int = 32):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, out_features),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        return torch.sigmoid(self.block(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ...........\n",
      "testing  ...........\n",
      "Fold 0 | Epoch 0 | train loss 0.55 | test loss 0.51 | test auroc 0.92\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 0 | Epoch 1 | train loss 0.48 | test loss 0.50 | test auroc 0.92\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 0 | Epoch 2 | train loss 0.45 | test loss 0.50 | test auroc 0.92\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 1 | Epoch 0 | train loss 0.47 | test loss 0.45 | test auroc 0.94\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 1 | Epoch 1 | train loss 0.45 | test loss 0.45 | test auroc 0.94\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 1 | Epoch 2 | train loss 0.43 | test loss 0.45 | test auroc 0.92\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 2 | Epoch 0 | train loss 0.45 | test loss 0.42 | test auroc 0.95\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 2 | Epoch 1 | train loss 0.43 | test loss 0.43 | test auroc 0.95\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 2 | Epoch 2 | train loss 0.42 | test loss 0.43 | test auroc 0.94\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9246694048741035, 0.942074215096939)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(len(dataset.vectorizer.get_feature_names_out()), 1).to(device)\n",
    "scores = cross_validate(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    num_folds=num_folds,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr\n",
    ")\n",
    "confidence_interval(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How was it this time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try one more time with regularization (prevent overfitting and improve generalization performance) using `Dropout`.\n",
    "`Dropout` will randomly turn off some nodes during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, hidden_units: int = 32):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_units, out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        return torch.sigmoid(self.block(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ...........\n",
      "testing  ...........\n",
      "Fold 0 | Epoch 0 | train loss 0.63 | test loss 0.55 | test auroc 0.91\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 0 | Epoch 1 | train loss 0.60 | test loss 0.53 | test auroc 0.93\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 0 | Epoch 2 | train loss 0.59 | test loss 0.52 | test auroc 0.92\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 1 | Epoch 0 | train loss 0.60 | test loss 0.50 | test auroc 0.94\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 1 | Epoch 1 | train loss 0.59 | test loss 0.49 | test auroc 0.93\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 1 | Epoch 2 | train loss 0.58 | test loss 0.50 | test auroc 0.91\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 2 | Epoch 0 | train loss 0.59 | test loss 0.48 | test auroc 0.94\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 2 | Epoch 1 | train loss 0.58 | test loss 0.48 | test auroc 0.95\n",
      "training ...........\n",
      "testing  ...........\n",
      "Fold 2 | Epoch 2 | train loss 0.58 | test loss 0.47 | test auroc 0.95\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9208162858122616, 0.9416881319202838)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(len(dataset.vectorizer.get_feature_names_out()), 1).to(device)\n",
    "scores = cross_validate(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    num_folds=num_folds,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr\n",
    ")\n",
    "confidence_interval(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    " As you can see the rest of the models didn't do that well. Machine learning requires lots of experimenting.\n",
    "It often requires trying out different models, hyperparameters, and preprocessing techniques to achieve optimal results.\n",
    "This is only the start of NLP. Throughout the workshop you may find other approaches to this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
